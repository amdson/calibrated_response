# Project Overview

## Objective
Calibrated Response is a Python framework for turning forecasting questions into calibrated probability distributions by decomposing the question into variables, eliciting LLM-guided queries, and combining the results with maximum entropy (see `README.md:1`). The workflow is: generate variables, generate queries about those variables, collect LLM answers, synthesize a joint distribution with MaxEnt, and evaluate calibration against resolved Metaculus questions (`README.md:7`).

## Architecture at a glance
The `Pipeline` class orchestrates the full process: it runs variable generation, query synthesis, query answering, and a MaxEnt distribution builder, then returns a `HistogramDistribution` plus diagnostic metadata (`calibrated_response/pipeline.py:1`).

## Directory map
- `calibrated_response/` houses the package. `pipeline.py` is the entry point for predictions (`calibrated_response/pipeline.py:1`).
- `calibrated_response/models/` defines the data contracts for questions, variables, distributions, and query/estimate specs; key classes include `Question`/`MetaculusQuestion`, typed `Variable` families, histogram-backed `Distribution`, and the query/estimate unions that drive constraints (`calibrated_response/models/question.py:1`, `calibrated_response/models/variable.py:1`, `calibrated_response/models/distribution.py:1`, `calibrated_response/models/query.py:1`).
- `calibrated_response/generation/` contains the LLM-backed generators: `VariableGenerator`, `QueryGenerator`, and `QueryAnswerer` translate a question into targeted variables, structured query specs, and parsed answers (`calibrated_response/generation/variable_generator.py:1`, `calibrated_response/generation/query_generator.py:1`, `calibrated_response/generation/query_answerer.py:1`).
- `calibrated_response/maxent/` implements the MaxEnt math: `DistributionBuilder`, domain normalization, constraint conversion, and the solver stack with soft constraints and diagnostics (`calibrated_response/maxent/distribution_builder.py:1`, `calibrated_response/maxent/constraints.py:1`, `calibrated_response/maxent/solver.py:1`, `calibrated_response/maxent/multivariate_solver.py:1`).
- `calibrated_response/llm/` provides the LLM abstraction layer, including the abstract `LLMClient`, the `GeminiClient`, response parsing heuristics, and other client implementations (`calibrated_response/llm/base.py:1`, `calibrated_response/llm/gemini.py:1`, `calibrated_response/llm/response_parser.py:1`).
- `calibrated_response/evaluation/` supports experiments: `DatasetLoader` reads JSONL Metaculus dumps, `EvaluationRunner` drives the pipeline across resolved questions, and `CalibrationMetrics` scores calibration with CRPS/ECE/log scores (`calibrated_response/evaluation/loader.py:1`, `calibrated_response/evaluation/runner.py:1`, `calibrated_response/evaluation/metrics.py:1`).
- `calibrated_response/utils/` centralizes configuration loading from YAML or environment variables (`calibrated_response/utils/config.py:1`).
- `calibrated_response/visualization/` offers utilities such as pairwise marginal plots to inspect joint distributions (`calibrated_response/visualization/pairplot.py:1`).
- `examples/` contains runnable notebooks and scripts (e.g., `simple_prediction.py`, `run_evaluation.py`) that demonstrate variable/query generation and MaxEnt pipelines.
- `eval_scraper.py` gathers resolved Metaculus questions and histories so the evaluation runner has fresh data (`eval_scraper.py:1`).
- Notebooks (`debug.ipynb`, `math.ipynb`, `maxentr.ipynb`) plus `notes.txt` capture exploratory analysis and experimentation notes.
- `calibrated_response.egg-info/` holds packaging metadata generated by the editable install.

## Core abstractions
- **Pipeline** (`calibrated_response/pipeline.py:1`): orchestrates the end-to-end prediction, keeps track of question/context metadata, stores query results, and delegates to `DistributionBuilder` for the final histogram.
- **Data models** (`calibrated_response/models/question.py:1`, `calibrated_response/models/variable.py:1`, `calibrated_response/models/distribution.py:1`, `calibrated_response/models/query.py:1`): define the shape of questions, variables, distributions, propositions, and estimates that the whole stack manipulates.
- **Generation layer** (`calibrated_response/generation/variable_generator.py:1`, `calibrated_response/generation/query_generator.py:1`, `calibrated_response/generation/query_answerer.py:1`): wraps LLM calls (+ prompts) to produce sat-structured variables, queries, and parsed answers.
- **MaxEnt layer** (`calibrated_response/maxent/distribution_builder.py:1`, `calibrated_response/maxent/constraints.py:1`, `calibrated_response/maxent/solver.py:1`, `calibrated_response/maxent/multivariate_solver.py:1`): normalizes variable domains, converts estimates into constraints, and solves soft-constrained optimization problems to return marginals and diagnostics.
- **LLM layer** (`calibrated_response/llm/base.py:1`, `calibrated_response/llm/gemini.py:1`, `calibrated_response/llm/response_parser.py:1`): abstracts querying, structured responses, and parsing heuristics so the generation/answering components stay agnostic to the provider.
- **Evaluation flow** (`calibrated_response/evaluation/loader.py:1`, `calibrated_response/evaluation/runner.py:1`, `calibrated_response/evaluation/metrics.py:1`): loads Metaculus data, runs the pipeline per question, serializes predictions, and reports scores like CRPS, coverage, and ECE.
- **Configuration** (`calibrated_response/utils/config.py:1`): centralizes defaults for LLMs, pipeline settings, MaxEnt solver, and evaluation targets while providing YAML/env loaders.
- **Visualization helper** (`calibrated_response/visualization/pairplot.py:1`): plots diagonals/heatmaps of marginals and overlays constraint-conditioned summaries for inspection.
- **Dataset provisioning** (`eval_scraper.py:1`): scripts that fetch Metaculus questions to feed into the evaluation loader.
